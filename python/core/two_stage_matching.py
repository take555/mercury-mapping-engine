"""
2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
Stage 1: åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®š
Stage 2: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æ
"""

import time
import logging
from collections import defaultdict
import re

# ===============================================
# Stage 1: åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®šã‚·ã‚¹ãƒ†ãƒ 
# ===============================================

def identify_key_fields(headers_a, headers_b):
    """é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è‡ªå‹•ç‰¹å®š"""
    key_patterns = {
        'name': ['name', 'åå‰', 'ã‚«ãƒ¼ãƒ‰å', 'å•†å“å', 'title', 'product'],
        'id': ['id', 'serial', 'å‹ç•ª', 'code', 'number', 'jan', 'sku'],
        'date': ['date', 'æ—¥ä»˜', 'ç™ºå£²æ—¥', 'release', 'publish', 'launch']
    }
    
    key_fields = {'a': {}, 'b': {}}
    
    for header in headers_a:
        header_lower = header.lower()
        for key_type, patterns in key_patterns.items():
            if any(pattern in header_lower for pattern in patterns):
                if key_type not in key_fields['a']:
                    key_fields['a'][key_type] = []
                key_fields['a'][key_type].append(header)
    
    for header in headers_b:
        header_lower = header.lower()
        for key_type, patterns in key_patterns.items():
            if any(pattern in header_lower for pattern in patterns):
                if key_type not in key_fields['b']:
                    key_fields['b'][key_type] = []
                key_fields['b'][key_type].append(header)
    
    return key_fields

def normalize_value(value, field_type):
    """å€¤ã®æ­£è¦åŒ–å‡¦ç†"""
    if not value:
        return ""
    
    value = str(value).strip().replace('\ufeff', '')
    
    if field_type == 'date':
        # æ—¥ä»˜ã®æ­£è¦åŒ–: 2024/1/24 â†’ 20240124
        if '/' in value:
            parts = value.split('/')
            if len(parts) == 3 and all(p.isdigit() for p in parts):
                year, month, day = parts
                return f"{year.zfill(4)}{month.zfill(2)}{day.zfill(2)}"
        return value.replace('-', '').replace('/', '').replace(' ', '')
    
    elif field_type == 'name':
        # åå‰ã®æ­£è¦åŒ–: è¨˜å·çµ±ä¸€ã€å¤§å°æ–‡å­—çµ±ä¸€
        return value.replace('ï¼†', '&').replace('ã€€', ' ').lower().strip()
    
    elif field_type == 'id':
        # IDã®æ­£è¦åŒ–: å¤§æ–‡å­—çµ±ä¸€ã€ç©ºç™½é™¤å»
        return value.upper().strip().replace(' ', '')
    
    return value.lower().strip()

def find_identical_cards(data_a, data_b, key_fields):
    """åŒä¸€ã‚«ãƒ¼ãƒ‰ãƒšã‚¢ã‚’ç‰¹å®š"""
    logger = logging.getLogger('identical_cards')
    
    def calculate_match_score(card_a, card_b):
        """ã‚«ãƒ¼ãƒ‰é–“ã®ãƒãƒƒãƒã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆã‚«ãƒ¼ãƒ‰åæœ€å„ªå…ˆï¼‰"""
        score = 0.0
        matches = []
        
        # ã‚«ãƒ¼ãƒ‰åã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯ï¼ˆAç¤¾ãƒ»Bç¤¾å…¥ã‚Œæ›¿ã‚ã‚Šå¯¾å¿œï¼‰
        name_matched = False
        fields_a_name = key_fields.get('a', {}).get('name', [])
        fields_b_name = key_fields.get('b', {}).get('name', [])
        
        for field_a in fields_a_name:
            for field_b in fields_b_name:
                val_a = normalize_value(card_a.get(field_a), 'name')
                val_b = normalize_value(card_b.get(field_b), 'name')
                
                if val_a and val_b and val_a == val_b:
                    score += 1.0  # åå‰ä¸€è‡´ã¯100ç‚¹ï¼ˆæœ€é‡è¦ï¼‰
                    name_matched = True
                    matches.append({
                        'type': 'name',
                        'field_a': field_a,
                        'field_b': field_b,
                        'value': val_a
                    })
                    break
            if name_matched:
                break
        
        # åå‰ãŒä¸€è‡´ã—ãŸå ´åˆã®ã¿ã€æ—¥ä»˜ã‚’ãƒœãƒ¼ãƒŠã‚¹ã¨ã—ã¦è¿½åŠ 
        # æ³¨æ„: IDãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯åˆ¤å®šçµæœã¨ã—ã¦æ±ºå®šã•ã‚Œã‚‹ãŸã‚ã€ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®šã«ã¯ä½¿ç”¨ã—ãªã„
        if name_matched:
            for key_type in ['date']:
                fields_a = key_fields.get('a', {}).get(key_type, [])
                fields_b = key_fields.get('b', {}).get(key_type, [])
                
                for field_a in fields_a:
                    for field_b in fields_b:
                        val_a = normalize_value(card_a.get(field_a), key_type)
                        val_b = normalize_value(card_b.get(field_b), key_type)
                        
                        if val_a and val_b and val_a == val_b:
                            # ãƒœãƒ¼ãƒŠã‚¹ç‚¹ã‚’è¿½åŠ 
                            if key_type == 'date':
                                score += 0.1   # æ—¥ä»˜ãƒœãƒ¼ãƒŠã‚¹: 10ç‚¹
                            
                            matches.append({
                                'type': key_type,
                                'field_a': field_a,
                                'field_b': field_b,
                                'value': val_a
                            })
                            break  # åŒã‚¿ã‚¤ãƒ—ã§è¤‡æ•°ãƒãƒƒãƒã—ã¦ã‚‚1å›ã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
        
        return score, matches
    
    # å®Ÿéš›ã®ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
    identical_pairs = []
    logger.info(f"åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®šé–‹å§‹: {len(data_a)}Ã—{len(data_b)}è¡Œ")
    
    for card_a in data_a:
        for card_b in data_b:
            score, match_details = calculate_match_score(card_a, card_b)
            
            # ã‚¹ã‚³ã‚¢1.0ä»¥ä¸Šã‚’åŒä¸€ã‚«ãƒ¼ãƒ‰ã¨ã—ã¦åˆ¤å®šï¼ˆã‚«ãƒ¼ãƒ‰åå¿…é ˆï¼‰
            if score >= 1.0:
                identical_pairs.append({
                    'card_a': card_a,
                    'card_b': card_b,
                    'match_score': round(score, 3),
                    'match_details': match_details
                })
    
    logger.info(f"åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®šå®Œäº†: {len(identical_pairs)}çµ„")
    
    # ãƒ‡ãƒ¼ã‚¿ç®¡ç†ç²’åº¦ã®é•ã„ã«å¯¾å¿œï¼šåŒä¸€ã‚«ãƒ¼ãƒ‰ã®çµ±åˆ
    consolidated_pairs = consolidate_identical_cards(identical_pairs, logger)
    logger.info(f"ã‚«ãƒ¼ãƒ‰çµ±åˆå¾Œ: {len(consolidated_pairs)}çµ„")
    
    return consolidated_pairs

def consolidate_identical_cards(identical_pairs, logger):
    """åŒä¸€ã‚«ãƒ¼ãƒ‰ã®çµ±åˆï¼ˆãƒ‡ãƒ¼ã‚¿ç®¡ç†ç²’åº¦ã®é•ã„ã«å¯¾å¿œï¼‰"""
    from collections import defaultdict
    
    # ã‚«ãƒ¼ãƒ‰åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    card_groups = defaultdict(list)
    
    for pair in identical_pairs:
        # ã‚«ãƒ¼ãƒ‰åã‚’å–å¾—ï¼ˆAç¤¾ãƒ»Bç¤¾ã®ã©ã¡ã‚‰ã‹ã‚‰ã§ã‚‚ï¼‰
        card_a = pair['card_a']
        card_b = pair['card_b']
        
        # ã‚«ãƒ¼ãƒ‰åã®å€™è£œãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è©¦è¡Œ
        name_candidates_a = ['ã‚«ãƒ¼ãƒ‰å', 'name', 'å•†å“å', 'title', 'product']
        name_candidates_b = ['name', 'name_short', 'ã‚«ãƒ¼ãƒ‰å', 'å•†å“å', 'title']
        
        card_name = None
        for field in name_candidates_a:
            if field in card_a and card_a[field]:
                card_name = str(card_a[field]).strip()
                break
        
        if not card_name:
            for field in name_candidates_b:
                if field in card_b and card_b[field]:
                    card_name = str(card_b[field]).strip()
                    break
        
        if card_name:
            # æ­£è¦åŒ–ã—ãŸã‚«ãƒ¼ãƒ‰åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            normalized_name = normalize_value(card_name, 'name')
            card_groups[normalized_name].append(pair)
    
    consolidated_pairs = []
    
    for normalized_name, group in card_groups.items():
        if len(group) == 1:
            # ã‚°ãƒ«ãƒ¼ãƒ—ã«1ã¤ã ã‘ã®å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
            consolidated_pairs.append(group[0])
        else:
            # è¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€ã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã®ãƒšã‚¢ã‚’ä»£è¡¨ã¨ã—ã¦é¸æŠ
            best_pair = max(group, key=lambda x: x['match_score'])
            logger.info(f"ã‚«ãƒ¼ãƒ‰ '{normalized_name}' ã®{len(group)}ä»¶ã‚’çµ±åˆ â†’ æœ€é«˜ã‚¹ã‚³ã‚¢{best_pair['match_score']}")
            consolidated_pairs.append(best_pair)
    
    return consolidated_pairs

# ===============================================
# Stage 2: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# ===============================================

def analyze_field_mappings_from_pairs(identical_pairs, headers_a, headers_b):
    """åŒä¸€ã‚«ãƒ¼ãƒ‰ãƒšã‚¢ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å­¦ç¿’"""
    logger = logging.getLogger('field_mapping')
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å€¤ã®ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é›†è¨ˆ
    field_match_stats = defaultdict(lambda: {
        'exact_matches': 0,
        'total_comparisons': 0,
        'sample_values': []
    })
    
    logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°å­¦ç¿’é–‹å§‹: {len(identical_pairs)}çµ„ã®ãƒšã‚¢")
    
    for pair in identical_pairs:
        card_a = pair['card_a']
        card_b = pair['card_b']
        
        # å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰çµ„ã¿åˆã‚ã›ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆåŒä¸€ã‚«ãƒ¼ãƒ‰ãƒšã‚¢ãªã®ã§åŠ¹ç‡çš„ï¼‰
        for field_a in headers_a:
            for field_b in headers_b:
                val_a = str(card_a.get(field_a, '')).strip()
                val_b = str(card_b.get(field_b, '')).strip()
                
                field_pair = f"{field_a}â†’{field_b}"
                stats = field_match_stats[field_pair]
                
                stats['total_comparisons'] += 1
                
                # å€¤ãŒä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
                if val_a and val_b:
                    val_a_norm = normalize_for_comparison(val_a)
                    val_b_norm = normalize_for_comparison(val_b)
                    
                    if val_a_norm == val_b_norm:
                        stats['exact_matches'] += 1
                        
                        # ã‚µãƒ³ãƒ—ãƒ«å€¤ã‚’ä¿å­˜
                        if len(stats['sample_values']) < 3:
                            stats['sample_values'].append({
                                'original_a': val_a,
                                'original_b': val_b,
                                'normalized': val_a_norm
                            })
    
    # ä¿¡é ¼åº¦ã®é«˜ã„ãƒãƒƒãƒ”ãƒ³ã‚°ã®ã¿ã‚’æŠ½å‡º
    field_mappings = []
    
    for field_pair, stats in field_match_stats.items():
        if stats['total_comparisons'] > 0:
            confidence = stats['exact_matches'] / stats['total_comparisons']
            
            # ä¿¡é ¼åº¦50%ä»¥ä¸Šã®ãƒãƒƒãƒ”ãƒ³ã‚°ã®ã¿æ¡ç”¨
            if confidence >= 0.5:
                field_a, field_b = field_pair.split('â†’')
                
                # IDãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                field_a_clean = field_a.replace('\ufeff', '').strip()
                field_b_clean = field_b.replace('\ufeff', '').strip()
                is_id_field = is_id_field_mapping(field_a_clean, field_b_clean)
                
                field_mappings.append({
                    'field_a': field_a_clean,
                    'field_b': field_b_clean,
                    'confidence': round(confidence, 3),
                    'sample_count': stats['exact_matches'],
                    'total_comparisons': stats['total_comparisons'],
                    'field_type': 'id_mapping' if is_id_field else 'learned_from_identical_cards',
                    'quality_score': 'ID_Field' if is_id_field else ('High' if confidence > 0.8 else 'Medium'),
                    'sample_values': stats['sample_values'],
                    'is_id_field': is_id_field
                })
    
    # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
    field_mappings.sort(key=lambda x: x['confidence'], reverse=True)
    
    logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°å­¦ç¿’å®Œäº†: {len(field_mappings)}ä»¶")
    
    # å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚’è¿½åŠ å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆï¼‰
    cooccurrence_mappings = analyze_cooccurrence_patterns(identical_pairs, headers_a, headers_b, logger)
    
    # æ—¢å­˜ãƒãƒƒãƒ”ãƒ³ã‚°ã¨å…±èµ·ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’çµ±åˆ
    enhanced_mappings = merge_mappings(field_mappings, cooccurrence_mappings, logger)
    
    return enhanced_mappings

def is_id_field_mapping(field_a, field_b):
    """IDãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    id_patterns = [
        'id', 'ID', 'Id',
        'serial', 'Serial', 'SERIAL',
        'å‹ç•ª', 'æ£šç•ª', 'ã‚³ãƒ¼ãƒ‰', 'code', 'Code', 'CODE',
        'number', 'Number', 'NUMBER', 'No', 'no', 'NO',
        'jan', 'JAN', 'sku', 'SKU',
        'product_id', 'item_id', 'card_id',
        'product_code', 'item_code', 'card_code'
    ]
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«IDãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    field_a_lower = field_a.lower()
    field_b_lower = field_b.lower()
    
    for pattern in id_patterns:
        pattern_lower = pattern.lower()
        if pattern_lower in field_a_lower or pattern_lower in field_b_lower:
            return True
    
    return False

def analyze_cooccurrence_patterns(identical_pairs, headers_a, headers_b, logger):
    """åŒä¸€ã‚«ãƒ¼ãƒ‰ãƒšã‚¢é–“ã§ã®å€¤å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
    import math
    from collections import defaultdict, Counter
    
    logger.info("ğŸ” å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æé–‹å§‹...")
    
    cooccurrence_stats = {}
    
    for field_a in headers_a:
        for field_b in headers_b:
            # å€¤ãƒšã‚¢ã‚’åé›†
            value_pairs = []
            valid_pairs = 0
            
            for pair in identical_pairs:
                val_a = str(pair['card_a'].get(field_a, '')).strip()
                val_b = str(pair['card_b'].get(field_b, '')).strip()
                
                # ç©ºã§ãªã„å€¤ã®ãƒšã‚¢ã®ã¿åé›†
                if val_a and val_b and val_a != 'N/A' and val_b != 'N/A':
                    value_pairs.append((val_a, val_b))
                    valid_pairs += 1
            
            # ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿åˆ†æï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«é–¾å€¤ã‚’1ã«ï¼‰
            if valid_pairs >= 1:
                mutual_info = calculate_mutual_information(value_pairs)
                unique_patterns = len(set(value_pairs))
                
                cooccurrence_stats[f"{field_a}â†’{field_b}"] = {
                    'field_a': field_a,
                    'field_b': field_b,
                    'mutual_information': mutual_info,
                    'sample_count': valid_pairs,
                    'unique_patterns': unique_patterns,
                    'pattern_diversity': unique_patterns / valid_pairs,
                    'top_patterns': Counter(value_pairs).most_common(3)
                }
    
    # ç›¸äº’æƒ…å ±é‡ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’å–å¾—
    sorted_stats = sorted(cooccurrence_stats.items(), 
                         key=lambda x: x[1]['mutual_information'], reverse=True)
    
    cooccurrence_mappings = []
    for field_pair, stats in sorted_stats[:20]:  # ä¸Šä½20ä»¶
        # ç›¸äº’æƒ…å ±é‡ã‚’ä¿¡é ¼åº¦ã«å¤‰æ›ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        confidence = min(stats['mutual_information'] / 2.0, 1.0)
        
        if confidence > 0.3:  # é–¾å€¤0.3ä»¥ä¸Šã®ã¿æ¡ç”¨
            cooccurrence_mappings.append({
                'field_a': stats['field_a'],
                'field_b': stats['field_b'],
                'confidence': round(confidence, 3),
                'sample_count': stats['sample_count'],
                'total_comparisons': stats['sample_count'],
                'field_type': 'cooccurrence_pattern',
                'quality_score': 'Cooccurrence',
                'mutual_information': round(stats['mutual_information'], 3),
                'pattern_diversity': round(stats['pattern_diversity'], 3),
                'top_patterns': stats['top_patterns']
            })
    
    logger.info(f"âœ… å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†: {len(cooccurrence_mappings)}ä»¶æ¤œå‡º")
    return cooccurrence_mappings

def calculate_mutual_information(value_pairs):
    """ç›¸äº’æƒ…å ±é‡ã‚’è¨ˆç®—"""
    if not value_pairs:
        return 0.0
    
    from collections import Counter
    import math
    
    # å€¤ãƒšã‚¢ã®é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    pair_counts = Counter(value_pairs)
    total_pairs = len(value_pairs)
    
    # å€‹åˆ¥å€¤ã®é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    values_a = [pair[0] for pair in value_pairs]
    values_b = [pair[1] for pair in value_pairs]
    counts_a = Counter(values_a)
    counts_b = Counter(values_b)
    
    # ç›¸äº’æƒ…å ±é‡è¨ˆç®—
    mutual_info = 0.0
    for (val_a, val_b), joint_count in pair_counts.items():
        p_joint = joint_count / total_pairs
        p_a = counts_a[val_a] / total_pairs
        p_b = counts_b[val_b] / total_pairs
        
        if p_joint > 0:
            mutual_info += p_joint * math.log2(p_joint / (p_a * p_b))
    
    return max(0.0, mutual_info)

def merge_mappings(field_mappings, cooccurrence_mappings, logger):
    """æ—¢å­˜ãƒãƒƒãƒ”ãƒ³ã‚°ã¨å…±èµ·ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’çµ±åˆ"""
    existing_pairs = set(f"{m['field_a']}â†’{m['field_b']}" for m in field_mappings)
    
    # æ–°ã—ãç™ºè¦‹ã•ã‚ŒãŸå…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³
    new_mappings = []
    for mapping in cooccurrence_mappings:
        pair_key = f"{mapping['field_a']}â†’{mapping['field_b']}"
        if pair_key not in existing_pairs:
            new_mappings.append(mapping)
            logger.info(f"ğŸ†• æ–°ç™ºè¦‹: {mapping['field_a']} â†” {mapping['field_b']} "
                       f"(ä¿¡é ¼åº¦: {mapping['confidence']}, ç›¸äº’æƒ…å ±é‡: {mapping['mutual_information']})")
    
    # çµ±åˆã—ã¦ã‚½ãƒ¼ãƒˆ
    all_mappings = field_mappings + new_mappings
    all_mappings.sort(key=lambda x: x['confidence'], reverse=True)
    
    return all_mappings

def normalize_for_comparison(value):
    """æ¯”è¼ƒç”¨ã®å€¤æ­£è¦åŒ–"""
    if not value:
        return ""
    
    value = str(value).strip().replace('\ufeff', '')
    
    # æ—¥ä»˜æ­£è¦åŒ–
    if '/' in value and len(value.split('/')) == 3:
        parts = value.split('/')
        if len(parts) == 3 and all(p.isdigit() for p in parts):
            year, month, day = parts
            return f"{year.zfill(4)}{month.zfill(2)}{day.zfill(2)}"
    
    # ä¸€èˆ¬çš„ãªæ­£è¦åŒ–
    return value.replace('ï¼†', '&').replace('ã€€', ' ').lower().strip()

# ===============================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼š2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
# ===============================================

def two_stage_matching_system(data_a, data_b, headers_a, headers_b):
    """åŠ¹ç‡çš„ãª2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    logger = logging.getLogger('two_stage_matching')
    start_time = time.time()
    
    logger.info("=" * 50)
    logger.info("ğŸš€ 2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    logger.info("=" * 50)
    
    # Stage 1: åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®š
    logger.info("ğŸ¯ Stage 1: åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®š")
    stage1_start = time.time()
    
    key_fields = identify_key_fields(headers_a, headers_b)
    logger.info(f"ç‰¹å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {key_fields}")
    
    identical_pairs = find_identical_cards(data_a, data_b, key_fields)
    
    stage1_time = time.time() - stage1_start
    logger.info(f"âœ… Stage 1å®Œäº†: {len(identical_pairs)}çµ„ ({stage1_time:.2f}ç§’)")
    
    if not identical_pairs:
        logger.warning("åŒä¸€ã‚«ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å¾“æ¥ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return [], []
    
    # Stage 2: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æ
    logger.info("ğŸ”— Stage 2: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æ")
    stage2_start = time.time()
    
    field_mappings = analyze_field_mappings_from_pairs(identical_pairs, headers_a, headers_b)
    
    stage2_time = time.time() - stage2_start
    logger.info(f"âœ… Stage 2å®Œäº†: {len(field_mappings)}ä»¶ ({stage2_time:.2f}ç§’)")
    
    # çµæœã‚µãƒãƒªãƒ¼
    total_time = time.time() - start_time
    logger.info("=" * 50)
    logger.info(f"ğŸ 2æ®µéšãƒãƒƒãƒãƒ³ã‚°å®Œäº†: {total_time:.2f}ç§’")
    logger.info(f"ğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
    logger.info(f"   - åŒä¸€ã‚«ãƒ¼ãƒ‰: {len(identical_pairs)}çµ„")
    logger.info(f"   - ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°: {len(field_mappings)}ä»¶")
    logger.info(f"   - é«˜ä¿¡é ¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°: {len([m for m in field_mappings if m['confidence'] > 0.8])}ä»¶")
    logger.info("=" * 50)
    
    return identical_pairs, field_mappings

# ===============================================
# enhanced.pyçµ±åˆç”¨ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
# ===============================================

def enhanced_two_stage_matching(data_a, data_b, headers_a, headers_b, max_sample_size=100):
    """enhanced.pyç”¨ã®2æ®µéšãƒãƒƒãƒãƒ³ã‚°"""
    logger = logging.getLogger('enhanced_matching')
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºåˆ¶é™
    if len(data_a) > max_sample_size:
        data_a = data_a[:max_sample_size]
        logger.info(f"Aç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’{max_sample_size}ä»¶ã«åˆ¶é™")
    
    if len(data_b) > max_sample_size:
        data_b = data_b[:max_sample_size]
        logger.info(f"Bç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’{max_sample_size}ä»¶ã«åˆ¶é™")
    
    # 2æ®µéšãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
    identical_pairs, field_mappings = two_stage_matching_system(
        data_a, data_b, headers_a, headers_b
    )
    
    # enhanced.pyã®æ—¢å­˜å½¢å¼ã«åˆã‚ã›ã¦çµæœã‚’å¤‰æ›
    matches = []
    for pair in identical_pairs:
        similarity_details = {}
        for detail in pair['match_details']:
            field_pair = f"{detail['field_a']}â†’{detail['field_b']}"
            similarity_details[field_pair] = 1.0
        
        matches.append({
            'card_a': pair['card_a'],
            'card_b': pair['card_b'],
            'overall_similarity': pair['match_score'],
            'similarity_details': similarity_details
        })
    
    return matches, field_mappings