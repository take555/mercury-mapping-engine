# enhanced.py ã® _handle_enhanced_analysis_post ã«è©³ç´°ãƒ­ã‚°ã‚’è¿½åŠ 

def _handle_enhanced_analysis_post():
    """POSTå‡¦ç† - è©³ç´°ãƒ­ã‚°ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨åˆ†æå®Ÿè¡Œ"""
    performance_logger.start_timer('enhanced_analysis_page')
    
    try:
        analysis_logger.logger.info("=" * 60)
        analysis_logger.logger.info("ğŸš€ ENHANCED ANALYSIS START")
        analysis_logger.logger.info("=" * 60)
        
        # ãƒ•ã‚©ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿å–å¾—
        similarity_mode = request.form.get('similarity_mode', 'library')
        max_sample_size = int(request.form.get('max_sample_size', 100))
        full_analysis = request.form.get('full_analysis') == 'on'
        ai_model = request.form.get('ai_model', 'claude-3-haiku-20240307')
        
        analysis_logger.logger.info(f"ğŸ“‹ è¨­å®šæƒ…å ±:")
        analysis_logger.logger.info(f"   - åˆ†æãƒ¢ãƒ¼ãƒ‰: {similarity_mode}")
        analysis_logger.logger.info(f"   - ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {max_sample_size}")
        analysis_logger.logger.info(f"   - ãƒ•ãƒ«åˆ†æ: {full_analysis}")
        analysis_logger.logger.info(f"   - AIãƒ¢ãƒ‡ãƒ«: {ai_model}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        analysis_logger.logger.info("ğŸ“ Step 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†é–‹å§‹")
        file_a = request.files.get('file_a')
        file_b = request.files.get('file_b')
        
        if not file_a or not file_b:
            analysis_logger.logger.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: ä¸¡æ–¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦")
            return _render_error_page("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼", "ä¸¡æ–¹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
        
        analysis_logger.logger.info(f"   - ãƒ•ã‚¡ã‚¤ãƒ«A: {file_a.filename} ({file_a.content_length} bytes)")
        analysis_logger.logger.info(f"   - ãƒ•ã‚¡ã‚¤ãƒ«B: {file_b.filename} ({file_b.content_length} bytes)")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        upload_dir = '/app/uploads'
        os.makedirs(upload_dir, exist_ok=True)
        
        file_a_path = os.path.join(upload_dir, f'uploaded_a_{int(time.time())}.csv')
        file_b_path = os.path.join(upload_dir, f'uploaded_b_{int(time.time())}.csv')
        
        file_a.save(file_a_path)
        file_b.save(file_b_path)
        analysis_logger.logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†")
        
        # æ–°ã—ã„MappingEngineã‚’ä½œæˆ
        analysis_logger.logger.info("ğŸ”§ Step 2: MappingEngineåˆæœŸåŒ–é–‹å§‹")
        config = Config.get_analysis_config()
        engine = create_mapping_engine(config)
        analysis_logger.logger.info("âœ… MappingEngineåˆæœŸåŒ–å®Œäº†")
        
        # AI ManageråˆæœŸåŒ–ï¼ˆAI modeã®å ´åˆï¼‰
        ai_manager = None
        if similarity_mode == 'ai':
            analysis_logger.logger.info("ğŸ¤– Step 2.1: AI ManageråˆæœŸåŒ–é–‹å§‹")
            try:
                from ai import create_ai_manager
                ai_config = {
                    'claude_config': {
                        'default_model': ai_model
                    }
                }
                ai_manager = create_ai_manager(ai_config)
                analysis_logger.logger.info(f"âœ… AI ManageråˆæœŸåŒ–å®Œäº†: {ai_model}")
            except Exception as e:
                analysis_logger.logger.error(f"âŒ AI ManageråˆæœŸåŒ–å¤±æ•—: {e}")
                return _render_error_page("AIè¨­å®šã‚¨ãƒ©ãƒ¼", f"Claude AI ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        
        # CSVåˆ†æ
        analysis_logger.logger.info("ğŸ“Š Step 3: CSVåˆ†æé–‹å§‹")
        start_time = time.time()
        
        csv_result = engine.analyze_csv_files(file_a_path, file_b_path, full_analysis=full_analysis)
        
        csv_time = time.time() - start_time
        analysis_logger.logger.info(f"âœ… CSVåˆ†æå®Œäº† ({csv_time:.2f}ç§’)")

        if 'error' in csv_result:
            analysis_logger.logger.error(f"âŒ CSVåˆ†æã‚¨ãƒ©ãƒ¼: {csv_result['error']}")
            return _render_error_page("CSVåˆ†æã‚¨ãƒ©ãƒ¼", csv_result['error'])
        
        analysis_a = csv_result['analysis_a']
        analysis_b = csv_result['analysis_b']
        
        analysis_logger.logger.info(f"ğŸ“‹ CSVåˆ†æçµæœ:")
        analysis_logger.logger.info(f"   - Aç¤¾: {len(analysis_a['headers'])}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰, {analysis_a['total_rows']}è¡Œ")
        analysis_logger.logger.info(f"   - Bç¤¾: {len(analysis_b['headers'])}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰, {analysis_b['total_rows']}è¡Œ")
        analysis_logger.logger.info(f"   - Aç¤¾ãƒ˜ãƒƒãƒ€ãƒ¼: {analysis_a['headers'][:5]}...")
        analysis_logger.logger.info(f"   - Bç¤¾ãƒ˜ãƒƒãƒ€ãƒ¼: {analysis_b['headers'][:5]}...")
        
        # ğŸ”¥ Brute Force Matchingå®Ÿè¡Œï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
        analysis_logger.logger.info("ğŸ’ª Step 4: Brute Force Matchingé–‹å§‹")
        start_time = time.time()
        
        try:
            card_matcher = engine.card_matcher
            
            data_a = analysis_a.get('full_data', analysis_a['sample_data'])
            data_b = analysis_b.get('full_data', analysis_b['sample_data'])
            
            analysis_logger.logger.info(f"ğŸ“Š ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ãƒ‡ãƒ¼ã‚¿:")
            analysis_logger.logger.info(f"   - Aç¤¾ãƒ‡ãƒ¼ã‚¿: {len(data_a)}è¡Œ")
            analysis_logger.logger.info(f"   - Bç¤¾ãƒ‡ãƒ¼ã‚¿: {len(data_b)}è¡Œ")
            analysis_logger.logger.info(f"   - äºˆæƒ³æ¯”è¼ƒå›æ•°: {len(data_a)} Ã— {len(data_b)} = {len(data_a) * len(data_b)}")
            
            matches = card_matcher.brute_force_matching(
                data_a,
                data_b,
                analysis_a['headers'],
                analysis_b['headers'],
                max_sample_size=max_sample_size,
                similarity_mode=similarity_mode,
                ai_manager=ai_manager
            )
            
            matching_time = time.time() - start_time
            analysis_logger.logger.info(f"âœ… Brute Force Matchingå®Œäº† ({matching_time:.2f}ç§’)")
            analysis_logger.logger.info(f"ğŸ¯ ãƒãƒƒãƒãƒ³ã‚°çµæœ: {len(matches)}ä»¶")
            
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
            analysis_logger.logger.info("ğŸ”— Step 5: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æé–‹å§‹")
            start_time = time.time()
            
            if matches:
                try:
                    analysis_logger.logger.info(f"   - å…¥åŠ›ãƒãƒƒãƒæ•°: {len(matches)}")
                    
                    field_mapping_result = engine.field_mapper.analyze_field_mappings_from_matches(
                        matches, analysis_a['headers'], analysis_b['headers']
                    )
                    
                    analysis_logger.logger.info(f"   - ãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚¿ã‚¤ãƒ—: {type(field_mapping_result)}")
                    
                    # è¿”ã‚Šå€¤ã®å‹ã«å¿œã˜ã¦é©åˆ‡ã«å‡¦ç†ï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
                    if isinstance(field_mapping_result, tuple):
                        enhanced_mappings = field_mapping_result[0] if field_mapping_result else []
                        analysis_logger.logger.info(f"   - Tupleå½¢å¼: è¦ç´ æ•°={len(field_mapping_result)}, ç¬¬1è¦ç´ ={len(enhanced_mappings)}ä»¶")
                    elif isinstance(field_mapping_result, dict):
                        analysis_logger.logger.info(f"   - Dictå½¢å¼: ã‚­ãƒ¼={list(field_mapping_result.keys())[:5]}")
                        
                        if 'mappings' in field_mapping_result:
                            enhanced_mappings = field_mapping_result['mappings']
                            analysis_logger.logger.info(f"   - 'mappings'ã‚­ãƒ¼ã‹ã‚‰å–å¾—: {len(enhanced_mappings)}ä»¶")
                        elif 'enhanced_mappings' in field_mapping_result:
                            enhanced_mappings = field_mapping_result['enhanced_mappings']
                            analysis_logger.logger.info(f"   - 'enhanced_mappings'ã‚­ãƒ¼ã‹ã‚‰å–å¾—: {len(enhanced_mappings)}ä»¶")
                        else:
                            # dictã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                            enhanced_mappings = [
                                {
                                    'field_a': k.split('â†’')[0] if 'â†’' in k else k,
                                    'field_b': k.split('â†’')[1] if 'â†’' in k else 'unknown',
                                    'confidence': v.get('confidence', v.get('avg_similarity', 0.0)) if isinstance(v, dict) else 0.0,
                                    'sample_count': v.get('count', v.get('sample_count', 1)) if isinstance(v, dict) else 1,
                                    'field_type': v.get('field_type', 'unknown') if isinstance(v, dict) else 'unknown'
                                }
                                for k, v in field_mapping_result.items()
                            ]
                            analysis_logger.logger.info(f"   - Dictå¤‰æ›: {len(enhanced_mappings)}ä»¶ã®ãƒãƒƒãƒ”ãƒ³ã‚°")
                    elif isinstance(field_mapping_result, list):
                        enhanced_mappings = field_mapping_result
                        analysis_logger.logger.info(f"   - Listå½¢å¼: {len(enhanced_mappings)}ä»¶")
                    else:
                        analysis_logger.logger.warning(f"   - æœªçŸ¥ã®å½¢å¼: {type(field_mapping_result)}")
                        enhanced_mappings = []
                        
                except Exception as e:
                    analysis_logger.logger.error(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    analysis_logger.logger.error(f"   - ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
                    enhanced_mappings = []
                    
            else:
                enhanced_mappings = []
                analysis_logger.logger.info("   - ãƒãƒƒãƒãªã—: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¹ã‚­ãƒƒãƒ—")
            
            mapping_time = time.time() - start_time
            analysis_logger.logger.info(f"âœ… ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æå®Œäº† ({mapping_time:.2f}ç§’)")
            
            card_analysis_success = True
            
        except Exception as e:
            analysis_logger.logger.error(f"âŒ Brute Forceåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            analysis_logger.logger.error(f"   - ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
            enhanced_mappings = []
            matches = []
            card_analysis_success = False
            card_analysis_error = str(e)
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
        analysis_logger.logger.info("ğŸ“‹ Step 6: ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ä½œæˆé–‹å§‹")
        start_time = time.time()
        
        if enhanced_mappings:
            try:
                analysis_logger.logger.info(f"   - enhanced_mappings: {len(enhanced_mappings)}ä»¶")
                analysis_logger.logger.info(f"   - matches: {len(matches)}ä»¶")
                
                mapping_summary = engine.create_mapping_summary(enhanced_mappings, matches, analysis_a, analysis_b)
                validation_result = engine.validate_mapping_results(enhanced_mappings, matches)
                
                analysis_logger.logger.info("âœ… ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ä½œæˆå®Œäº†")
            except Exception as e:
                analysis_logger.logger.error(f"âŒ ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                analysis_logger.logger.error(f"   - ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
                mapping_summary = None
                validation_result = None
        else:
            mapping_summary = None
            validation_result = None
            analysis_logger.logger.info("   - ãƒãƒƒãƒ”ãƒ³ã‚°ãªã—: ã‚µãƒãƒªãƒ¼ä½œæˆã‚¹ã‚­ãƒƒãƒ—")
        
        summary_time = time.time() - start_time
        analysis_logger.logger.info(f"âœ… ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼å®Œäº† ({summary_time:.2f}ç§’)")
        
        # HTMLç”Ÿæˆï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
        analysis_logger.logger.info("ğŸ¨ Step 7: HTMLç”Ÿæˆé–‹å§‹")
        start_time = time.time()
        
        try:
            html = _build_enhanced_analysis_html(
                analysis_a, analysis_b, enhanced_mappings, matches,
                card_analysis_success, locals().get('card_analysis_error'),
                mapping_summary, validation_result, ai_model, similarity_mode
            )
            
            html_time = time.time() - start_time
            analysis_logger.logger.info(f"âœ… HTMLç”Ÿæˆå®Œäº† ({html_time:.2f}ç§’)")
            
        except Exception as e:
            analysis_logger.logger.error(f"âŒ HTMLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            analysis_logger.logger.error(f"   - ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
            return _render_error_page("HTMLç”Ÿæˆã‚¨ãƒ©ãƒ¼", str(e))
        
        total_time = performance_logger.end_timer('enhanced_analysis_page')
        analysis_logger.logger.info("=" * 60)
        analysis_logger.logger.info(f"ğŸ ENHANCED ANALYSIS COMPLETE - ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        analysis_logger.logger.info("=" * 60)
        
        return html
        
    except Exception as e:
        analysis_logger.logger.error("=" * 60)
        analysis_logger.logger.error(f"ğŸ’¥ CRITICAL ERROR: {e}")
        analysis_logger.logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        analysis_logger.logger.error("=" * 60)
        current_app.logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        return _render_error_page("ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼", str(e), traceback.format_exc())

