#!/usr/bin/env python3
"""
2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®Webã‚¢ãƒ—ãƒªé¢¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import csv
import time
import logging
from pathlib import Path
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, '/home/aktk/Projects/docker-projects/mercury-mapping-engine/python')

# ãƒ­ã‚¬ãƒ¼è¨­å®šï¼ˆWebã‚¢ãƒ—ãƒªã®ãƒ­ã‚°ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger('enhanced_web_test')

def simulate_enhanced_analysis():
    """enhanced.pyã®Webã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
    
    logger.info("=" * 60)
    logger.info("ğŸš€ ENHANCED ANALYSIS WEB TEST START")
    logger.info("=" * 60)
    
    # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    logger.info("ğŸ“ Step 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†é–‹å§‹")
    
    file_a_path = '/home/aktk/Projects/docker-projects/mercury-mapping-engine/hikitsugi/Youyadatasample.csv'
    file_b_path = '/home/aktk/Projects/docker-projects/mercury-mapping-engine/hikitsugi/Tay2datasample.csv'
    
    logger.info(f"   - ãƒ•ã‚¡ã‚¤ãƒ«A: Youyadatasample.csv")
    logger.info(f"   - ãƒ•ã‚¡ã‚¤ãƒ«B: Tay2datasample.csv")
    logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†")
    
    # Step 2: CSVåˆ†æã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    logger.info("ğŸ“Š Step 3: CSVåˆ†æé–‹å§‹")
    start_time = time.time()
    
    # CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(file_a_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        headers_a = reader.fieldnames
        data_a = list(reader)
    
    with open(file_b_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        headers_b = reader.fieldnames
        data_b = list(reader)
    
    csv_time = time.time() - start_time
    logger.info(f"âœ… CSVåˆ†æå®Œäº† ({csv_time:.2f}ç§’)")
    logger.info(f"ğŸ“‹ CSVåˆ†æçµæœ:")
    logger.info(f"   - Aç¤¾: {len(headers_a)}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰, {len(data_a)}è¡Œ")
    logger.info(f"   - Bç¤¾: {len(headers_b)}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰, {len(data_b)}è¡Œ")
    
    # Step 4: 2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    logger.info("ğŸš€ Step 4: 2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    start_time = time.time()
    
    # 2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å®Ÿè¡Œ
    from core.two_stage_matching import enhanced_two_stage_matching
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’100ã«åˆ¶é™ï¼ˆWebã‚¢ãƒ—ãƒªã®è¨­å®šã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    max_sample_size = 100
    
    logger.info(f"ğŸ“Š ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ãƒ‡ãƒ¼ã‚¿:")
    logger.info(f"   - Aç¤¾ãƒ‡ãƒ¼ã‚¿: {min(len(data_a), max_sample_size)}è¡Œ")
    logger.info(f"   - Bç¤¾ãƒ‡ãƒ¼ã‚¿: {min(len(data_b), max_sample_size)}è¡Œ")
    logger.info(f"   - æ–°æ‰‹æ³•: é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã§åŒä¸€ã‚«ãƒ¼ãƒ‰ç‰¹å®š â†’ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°å­¦ç¿’")
    
    # 2æ®µéšãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
    matches, enhanced_mappings = enhanced_two_stage_matching(
        data_a[:max_sample_size],
        data_b[:max_sample_size],
        headers_a,
        headers_b,
        max_sample_size=max_sample_size
    )
    
    matching_time = time.time() - start_time
    logger.info(f"âœ… 2æ®µéšãƒãƒƒãƒãƒ³ã‚°å®Œäº† ({matching_time:.2f}ç§’)")
    logger.info(f"ğŸ¯ çµæœ: {len(matches)}ä»¶ã®åŒä¸€ã‚«ãƒ¼ãƒ‰, {len(enhanced_mappings)}ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°")
    
    # Step 5: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã¯æ—¢ã«å®Œäº†
    logger.info("âœ… Step 5: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°åˆ†æã¯2æ®µéšãƒãƒƒãƒãƒ³ã‚°ã§å®Œäº†æ¸ˆã¿")
    logger.info(f"   - é«˜ä¿¡é ¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°: {len([m for m in enhanced_mappings if m.get('confidence', 0) > 0.8])}ä»¶")
    logger.info(f"   - ä¸­ä¿¡é ¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°: {len([m for m in enhanced_mappings if 0.5 <= m.get('confidence', 0) <= 0.8])}ä»¶")
    
    # Step 6: ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼
    logger.info("ğŸ“‹ Step 6: ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ä½œæˆé–‹å§‹")
    logger.info(f"   - enhanced_mappings: {len(enhanced_mappings)}ä»¶")
    logger.info(f"   - matches: {len(matches)}ä»¶")
    logger.info("âœ… ãƒãƒƒãƒ”ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ä½œæˆå®Œäº†")
    
    # Step 7: çµæœè¡¨ç¤º
    logger.info("ğŸ¨ Step 7: çµæœè¡¨ç¤ºç”Ÿæˆé–‹å§‹")
    start_time = time.time()
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿
    old_comparisons = len(data_a) * len(data_b) * len(headers_a) * len(headers_b)
    old_estimated_time = old_comparisons / 1000000 * 2  # 100ä¸‡æ¯”è¼ƒã§ç´„2ç§’ã¨ä»®å®š
    
    html_time = time.time() - start_time
    logger.info(f"âœ… çµæœè¡¨ç¤ºç”Ÿæˆå®Œäº† ({html_time:.2f}ç§’)")
    
    # ç·å®Ÿè¡Œæ™‚é–“
    total_time = csv_time + matching_time + html_time
    logger.info("=" * 60)
    logger.info(f"ğŸ ENHANCED ANALYSIS WEB TEST COMPLETE - ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    logger.info("=" * 60)
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š WEBã‚¢ãƒ—ãƒªæ€§èƒ½ãƒ†ã‚¹ãƒˆçµæœ")
    print("=" * 80)
    
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    print(f"ğŸ¯ åŒä¸€ã‚«ãƒ¼ãƒ‰: {len(matches)}çµ„")
    print(f"ğŸ”— ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°: {len(enhanced_mappings)}ä»¶")
    
    # é«˜å“è³ªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡¨ç¤º
    high_conf_mappings = [m for m in enhanced_mappings if m.get('confidence', 0) > 0.8]
    print(f"âœ… é«˜ä¿¡é ¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ>0.8ï¼‰: {len(high_conf_mappings)}ä»¶")
    
    if high_conf_mappings:
        print("\nğŸ”— é«˜ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°:")
        for i, mapping in enumerate(high_conf_mappings[:5], 1):
            print(f"  {i}. {mapping['field_a']} â†’ {mapping['field_b']} (ä¿¡é ¼åº¦: {mapping['confidence']:.3f})")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
    print(f"\nğŸ“ˆ æ€§èƒ½æ”¹å–„:")
    print(f"   å¾“æ¥æ‰‹æ³•äºˆæƒ³æ™‚é–“: {old_estimated_time:.1f}ç§’")
    print(f"   æ–°æ‰‹æ³•å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    print(f"   æ”¹å–„ç‡: {(1 - total_time/old_estimated_time) * 100:.1f}%")
    
    # æˆåŠŸåˆ¤å®š
    print(f"\nâœ… æˆåŠŸåˆ¤å®š:")
    
    success_criteria = []
    if total_time < 10:
        success_criteria.append("âœ… å‡¦ç†æ™‚é–“10ç§’ä»¥å†…")
    else:
        success_criteria.append("âŒ å‡¦ç†æ™‚é–“10ç§’ã‚’è¶…é")
    
    if len(enhanced_mappings) <= 50:
        success_criteria.append("âœ… ãƒãƒƒãƒ”ãƒ³ã‚°ä»¶æ•°50ä»¶ä»¥ä¸‹")
    else:
        success_criteria.append("âŒ ãƒãƒƒãƒ”ãƒ³ã‚°ä»¶æ•°50ä»¶è¶…é")
    
    if len(high_conf_mappings) >= 5:
        success_criteria.append("âœ… é«˜ä¿¡é ¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°5ä»¶ä»¥ä¸Š")
    else:
        success_criteria.append("âŒ é«˜ä¿¡é ¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°5ä»¶æœªæº€")
    
    for criteria in success_criteria:
        print(f"   {criteria}")
    
    # åŒä¸€ã‚«ãƒ¼ãƒ‰ã®ä¾‹
    if matches:
        print(f"\nğŸ¯ åŒä¸€ã‚«ãƒ¼ãƒ‰ã®ä¾‹ï¼ˆä¸Šä½3ä»¶ï¼‰:")
        for i, match in enumerate(matches[:3], 1):
            card_a = match['card_a']
            card_b = match['card_b']
            score = match['overall_similarity']
            
            name_a = card_a.get('name', card_a.get('serial', 'Unknown'))
            name_b = card_b.get('ã‚«ãƒ¼ãƒ‰å', card_b.get('å‹ç•ª', 'Unknown'))
            
            print(f"  {i}. ã‚¹ã‚³ã‚¢: {score:.3f} | Aç¤¾: {name_a} | Bç¤¾: {name_b}")
    
    print("=" * 80)
    
    # JSONçµæœã‚‚å‡ºåŠ›ï¼ˆWeb APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    result = {
        "status": "success",
        "execution_time": total_time,
        "identical_cards": len(matches),
        "field_mappings": len(enhanced_mappings),
        "high_confidence_mappings": len(high_conf_mappings),
        "performance_improvement": f"{(1 - total_time/old_estimated_time) * 100:.1f}%",
        "success_criteria": {
            "time_under_10s": total_time < 10,
            "mappings_under_50": len(enhanced_mappings) <= 50,
            "high_conf_over_5": len(high_conf_mappings) >= 5
        }
    }
    
    print(f"\nğŸ“‹ JSON Result:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    simulate_enhanced_analysis()