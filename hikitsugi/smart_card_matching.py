import re
from datetime import datetime
import logging

def smart_card_matching(data_a, data_b, headers_a, headers_b):
    """è³¢ã„ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯"""
    logger = logging.getLogger('smart_matching')
    
    # ã‚­ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©
    key_field_mappings = {
        'name_fields': {
            'a': ['name', 'name_short', 'pos_en_name'],
            'b': ['ã‚«ãƒ¼ãƒ‰å', 'ã‚«ãƒ¼ãƒ‰åã‚«ãƒŠ']
        },
        'id_fields': {
            'a': ['serial', 'id'],
            'b': ['å‹ç•ª', 'è‡ªç¤¾å‹ç•ª']
        },
        'date_fields': {
            'a': ['releace_date'],
            'b': ['ç™ºå£²æ—¥']
        },
        'series_fields': {
            'a': ['series'],
            'b': ['ã‚·ãƒªãƒ¼ã‚º']
        }
    }
    
    def normalize_date(date_str):
        """æ—¥ä»˜ã‚’æ­£è¦åŒ–"""
        if not date_str:
            return ""
        
        date_str = str(date_str).strip()
        
        # YYYY/M/D â†’ YYYYMMDD
        if '/' in date_str:
            try:
                parts = date_str.split('/')
                if len(parts) == 3:
                    year, month, day = parts
                    return f"{year.zfill(4)}{month.zfill(2)}{day.zfill(2)}"
            except:
                pass
        
        # YYYYMMDDå½¢å¼ã¯ãã®ã¾ã¾
        if re.match(r'^\d{8}$', date_str):
            return date_str
        
        return date_str
    
    def normalize_text(text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–"""
        if not text:
            return ""
        
        text = str(text).strip()
        # å…¨è§’ãƒ»åŠè§’çµ±ä¸€
        text = text.replace('ï¼†', '&').replace('ã€€', ' ')
        # ç‰¹æ®Šæ–‡å­—é™¤å»
        text = re.sub(r'[^\w\s&ãƒ»]', '', text)
        return text.lower()
    
    def extract_field_value(card, field_list):
        """ã‚«ãƒ¼ãƒ‰ã‹ã‚‰æŒ‡å®šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å€¤ã‚’æŠ½å‡º"""
        for field in field_list:
            if field in card and card[field]:
                return str(card[field]).strip()
        return ""
    
    def calculate_card_similarity(card_a, card_b):
        """ã‚«ãƒ¼ãƒ‰é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        similarity_score = 0.0
        matches = []
        
        # 1. åå‰ã®æ¯”è¼ƒï¼ˆæœ€é‡è¦ï¼‰
        name_a = extract_field_value(card_a, key_field_mappings['name_fields']['a'])
        name_b = extract_field_value(card_b, key_field_mappings['name_fields']['b'])
        
        if name_a and name_b:
            name_a_norm = normalize_text(name_a)
            name_b_norm = normalize_text(name_b)
            
            if name_a_norm == name_b_norm:
                similarity_score += 0.4  # 40ç‚¹
                matches.append(('name_exact', name_a, name_b, 1.0))
            elif name_a_norm in name_b_norm or name_b_norm in name_a_norm:
                similarity_score += 0.3  # 30ç‚¹
                matches.append(('name_partial', name_a, name_b, 0.75))
        
        # 2. ID/å‹ç•ªã®æ¯”è¼ƒï¼ˆæœ€é‡è¦ï¼‰
        id_a = extract_field_value(card_a, key_field_mappings['id_fields']['a'])
        id_b = extract_field_value(card_b, key_field_mappings['id_fields']['b'])
        
        if id_a and id_b:
            if id_a == id_b:
                similarity_score += 0.4  # 40ç‚¹
                matches.append(('id_exact', id_a, id_b, 1.0))
            elif id_a in id_b or id_b in id_a:
                similarity_score += 0.2  # 20ç‚¹
                matches.append(('id_partial', id_a, id_b, 0.5))
        
        # 3. ç™ºå£²æ—¥ã®æ¯”è¼ƒ
        date_a = extract_field_value(card_a, key_field_mappings['date_fields']['a'])
        date_b = extract_field_value(card_b, key_field_mappings['date_fields']['b'])
        
        if date_a and date_b:
            date_a_norm = normalize_date(date_a)
            date_b_norm = normalize_date(date_b)
            
            if date_a_norm == date_b_norm:
                similarity_score += 0.15  # 15ç‚¹
                matches.append(('date_exact', date_a, date_b, 1.0))
        
        # 4. ã‚·ãƒªãƒ¼ã‚ºã®æ¯”è¼ƒ
        series_a = extract_field_value(card_a, key_field_mappings['series_fields']['a'])
        series_b = extract_field_value(card_b, key_field_mappings['series_fields']['b'])
        
        if series_a and series_b:
            series_a_norm = normalize_text(series_a)
            series_b_norm = normalize_text(series_b)
            
            if series_a_norm == series_b_norm:
                similarity_score += 0.05  # 5ç‚¹
                matches.append(('series_exact', series_a, series_b, 1.0))
        
        return similarity_score, matches
    
    # å®Ÿéš›ã®ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
    logger.info(f"ğŸ¯ è³¢ã„ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹: {len(data_a)}Ã—{len(data_b)}è¡Œ")
    
    card_matches = []
    field_mappings = {}
    
    for i, card_a in enumerate(data_a):
        for j, card_b in enumerate(data_b):
            similarity_score, match_details = calculate_card_similarity(card_a, card_b)
            
            # é¡ä¼¼åº¦ãŒ60%ä»¥ä¸Šã®å ´åˆã«ãƒãƒƒãƒã¨ã—ã¦è¨˜éŒ²
            if similarity_score >= 0.6:
                match = {
                    'card_a': card_a,
                    'card_b': card_b,
                    'overall_similarity': round(similarity_score, 3),
                    'match_details': match_details,
                    'confidence': 'high' if similarity_score >= 0.8 else 'medium'
                }
                card_matches.append(match)
                
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¨˜éŒ²
                for match_type, val_a, val_b, field_sim in match_details:
                    if match_type == 'name_exact' or match_type == 'name_partial':
                        field_mappings['nameâ†’ã‚«ãƒ¼ãƒ‰å'] = field_mappings.get('nameâ†’ã‚«ãƒ¼ãƒ‰å', 0) + field_sim
                    elif match_type == 'id_exact' or match_type == 'id_partial':
                        field_mappings['serialâ†’å‹ç•ª'] = field_mappings.get('serialâ†’å‹ç•ª', 0) + field_sim
                    elif match_type == 'date_exact':
                        field_mappings['releace_dateâ†’ç™ºå£²æ—¥'] = field_mappings.get('releace_dateâ†’ç™ºå£²æ—¥', 0) + field_sim
                    elif match_type == 'series_exact':
                        field_mappings['seriesâ†’ã‚·ãƒªãƒ¼ã‚º'] = field_mappings.get('seriesâ†’ã‚·ãƒªãƒ¼ã‚º', 0) + field_sim
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—
    enhanced_mappings = []
    for field_pair, total_sim in field_mappings.items():
        field_a, field_b = field_pair.split('â†’')
        confidence = min(1.0, total_sim / len(card_matches)) if card_matches else 0.0
        
        enhanced_mappings.append({
            'field_a': field_a,
            'field_b': field_b,
            'confidence': round(confidence, 3),
            'sample_count': len([m for m in card_matches if any(
                field_pair in f"{detail[0]}" for detail in m['match_details']
            )]),
            'field_type': 'smart_detected',
            'quality_score': 'High' if confidence > 0.8 else 'Medium' if confidence > 0.5 else 'Low'
        })
    
    logger.info(f"âœ… è³¢ã„ãƒãƒƒãƒãƒ³ã‚°å®Œäº†: {len(card_matches)}ä»¶ã®ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒ, {len(enhanced_mappings)}ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°")
    
    # çµæœã®è©³ç´°ãƒ­ã‚°
    for i, match in enumerate(card_matches[:5]):  # ä¸Šä½5ä»¶ã‚’è¡¨ç¤º
        logger.info(f"   ãƒãƒƒãƒ{i+1}: {match['overall_similarity']:.3f} - {match['confidence']}")
        name_a = extract_field_value(match['card_a'], key_field_mappings['name_fields']['a'])
        name_b = extract_field_value(match['card_b'], key_field_mappings['name_fields']['b'])
        logger.info(f"     Aç¤¾: {name_a} | Bç¤¾: {name_b}")
    
    return card_matches, enhanced_mappings

def apply_smart_matching_to_enhanced():
    """enhanced.pyã«è³¢ã„ãƒãƒƒãƒãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹"""
    
    # enhanced.pyã®Step 4éƒ¨åˆ†ã‚’ä»¥ä¸‹ã«ç½®ãæ›ãˆ
    replacement_code = '''
    # Step 4: è³¢ã„ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
    emergency_logger.info("ğŸ§  Step 4: è³¢ã„ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹")
    step4_start = time.time()
    
    try:
        data_a = analysis_a.get('full_data', analysis_a['sample_data'])
        data_b = analysis_b.get('full_data', analysis_b['sample_data'])
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™
        if len(data_a) > max_sample_size:
            data_a = data_a[:max_sample_size]
        if len(data_b) > max_sample_size:
            data_b = data_b[:max_sample_size]
        
        emergency_logger.info(f"   - å®Ÿéš›ã®å‡¦ç†ãƒ‡ãƒ¼ã‚¿: Aç¤¾{len(data_a)}è¡Œ, Bç¤¾{len(data_b)}è¡Œ")
        
        # è³¢ã„ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
        matches, enhanced_mappings = smart_card_matching(
            data_a, data_b, 
            analysis_a['headers'], analysis_b['headers']
        )
        
        step4_time = time.time() - step4_start
        emergency_logger.info(f"âœ… è³¢ã„ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°å®Œäº† ({step4_time:.2f}ç§’)")
        emergency_logger.info(f"ğŸ¯ çµæœ: {len(matches)}ä»¶ã®ãƒãƒƒãƒ, {len(enhanced_mappings)}ä»¶ã®ãƒãƒƒãƒ”ãƒ³ã‚°")
        
        card_analysis_success = True
        
    except Exception as e:
        emergency_logger.error(f"âŒ è³¢ã„ãƒãƒƒãƒãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        matches = []
        enhanced_mappings = []
        card_analysis_success = False
        card_analysis_error = str(e)
    '''
    
    return replacement_code